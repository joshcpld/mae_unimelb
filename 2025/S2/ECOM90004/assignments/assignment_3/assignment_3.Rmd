---
title: "ECOM90004 Assignment 3"
output: html_document
author: "Josh Copeland (SID: 1444772)"
date: "`r Sys.Date()`"
---

```{r setup, include=FALSE}

library(tidyverse)
library(lubridate)
library(forecast)
library(urca)
library(rugarch)

theme_set(theme_minimal())

```

```{r setup_comment}

#Packages used:

# library(tidyverse)
# library(lubridate)
# library(forecast)
# library(urca)
#library(rugarch)

```

```{r setup_data}
data <- read_csv("Google.csv", show_col_type = FALSE) %>% 
  select(date = Date, price = Google) %>% 
  mutate(price_log = log(price)) %>% 
  mutate(returns = price_log - lag(price_log)) %>%   
  na.omit()

data_est <- data %>% slice(1:(n() - 10)) # Estimation sample

data_fcst <- data %>% slice((n() - 9):n()) # Testing sample

```
# Question 1

The analysis below indicates:

* Although the time series of daily Google returns is mean-reverting, it is unlikely to satsify the definition of a stationary time series given clear patterns of conditional (non-constant) variance.

* The distribution of daily Google Returns looks leptokurtic: over-concentrated at the mean and excess mass in the tails relative to the fitted normal distribution.

* With respect to the descriptive statistics we generate:
  
  * The mean is approximately zero (0.007) and the standard deviation is 0.0193, indicating an average dialy volatility of roughtly 1.9%. They also shows the distribution is slightly left-skewed (-0.241).
  
  * This conclusion about leptokurtosis is reinforced by the descriptive statistics we generate, specifically by the Kurtosis value of 7.04, significantly above the value of 3 in a normal distribution. 
  
These conclusions suggest it would not be sufficient to model this time series using an ARMA model given its homoskedasticity assumption, and hence an ARMA-(G)ARCH approach should be investiagted instead.

```{r q1}
ggplot(data_est, aes(x = date, y = returns)) + geom_line() + labs(title = "Google daily returns",x = "Date", y = "Log Returns")


ggplot(data_est, aes(returns)) +
  geom_histogram(aes(y = after_stat(density)), bins = 30, alpha = .6) +
  stat_function(fun = dnorm, args = with(data_est, list(mean = mean(returns, na.rm=TRUE), sd = sd(returns, na.rm=TRUE))), linewidth = 1) +
  labs(title = "Histogram of Daily Google Returns with Fitted Normal Distribution", x = "Returns", y = "Density")

stats <- data_est %>% summarise(
  Mean = mean(returns, na.rm = TRUE),
  SD = sd(returns, na.rm = TRUE),
  Skewness = mean((returns - mean(returns, na.rm=TRUE))^3, na.rm=TRUE) / sd(returns, na.rm=TRUE)^3,
  Kurtosis = mean((returns - mean(returns, na.rm=TRUE))^4, na.rm=TRUE) / sd(returns, na.rm=TRUE)^4
) %>%  print()
```


# Question 2

The `model_dow` object below create a dummy variable for each day of the week, with coefficients interpreted as deviations from the intercept (Friday). It shows there is no "day of the week" or "calender" effect on Google's returns because:

* The F-test's p-value is very large (0.8027), and therefore we cannot reject the null hypothesis that all weekday coefficients are jointly equal to zero.

* All of the inidividual coefficient p-values are very large, meaning their null hypothesises of being equal to zero cannot be rejected at any reasonable significance level.

```{r q2}
data_est <- data_est %>% 
  mutate(weekday = wday(date, label = TRUE, abbr = TRUE))

model_dow <- lm(returns ~ weekday, data = data_est, contrasts = list(weekday = "contr.treatment"))
summary(model_dow)
```


# Question 3

### (a)

An AR(9) model is selected because it minimised AICc among the candidate specifications (testing up to an AR(15) model), which indicates the best balance between model fit and parsimony. Additionally, the Ljung-Box test p-value (0.9647) is large, suggesting there is no remaining autocorrelation in the residuals, supporting this model further.


```{r q3a}
pmax <- 15
AICc_vals <- LBp_vals <- numeric(pmax + 1)

for (p in 0:pmax) {
  eq <- Arima(data_est$returns, order = c(p, 0, 0), include.mean = FALSE, method = "ML")
  AICc_vals[p + 1] <- eq$aicc
  LBp_vals[p + 1] <- Box.test(residuals(eq), lag = p + 4, type = "Ljung-Box", fitdf = p)$p.value
}

best_p <- which.min(AICc_vals) - 1
cat("Selected AR order p =", best_p, "\nAICc =", round(min(AICc_vals), 3),
    "\nLjung-Box p-value =", round(LBp_vals[best_p + 1], 4), "\n")
```

### (b)

The inverse roots of this model all lie strictly within the unit circle, which conforms the stationarity of this model. Therefore, an AR(9) process is a suitable conditional mean model for Google's returns.


```{r q3b}
p <- 9
ar9 <- Arima(na.omit(data_est$returns), order = c(p,0,0), include.mean = FALSE)
autoplot(ar9) + ggtitle("AR(9) Roots")
```


# Question 4

### (a)

The analysis below model's Google's returns using an AR(9)-GARCH(1,1) process, which can be represented as:

\begin{aligned}
\text{Mean equation:} \quad 
Y_t &= \phi_1 Y_{t-1} + \phi_2 Y_{t-2} + \cdots + \phi_9 Y_{t-9} + U_t, 
\\
\text{Conditional variance (GARCH(1,1)):} \quad 
\sigma_t^2 &= \omega + \alpha_1,U_{t-1}^2 + \gamma_1,\sigma_{t-1}^2.
\end{aligned}

Using the coefficients estimated below, these equations can be rewritten as: 

\begin{aligned}
Y_t &= -0.0239\,Y_{t-1} - 0.0185\,Y_{t-2} - 0.0369\,Y_{t-3} - 0.0300\,Y_{t-4} - 0.0487\,Y_{t-5} \\
    &\quad - 0.0215\,Y_{t-6} + 0.0399\,Y_{t-7} - 0.0254\,Y_{t-8} + 0.0255\,Y_{t-9} + U_t, \\
\\
\sigma_t^2 &= 0.0000 + 0.0609\,U_{t-1}^2 + 0.8891\,\sigma_{t-1}^2.
\end{aligned}



```{r q4a}
ar_garch_model <- ugarchfit(
  data = data_est$returns,
  ugarchspec(
    variance.model = list(garchOrder = c(1, 1)),
    mean.model     = list(armaOrder = c(p, 0), include.mean = FALSE)))

coef_table <- round(coef(ar_garch_model), 4) %>%  print()
```

### (b)

After fitting an ARMA-(G)ARCH model the Weighted ARCH LM tests examine whether there is any remaining ARCH structure in the residuals. The p-values for these tests are very large (0.6985, 0.9748, 0.9912 at lags 3, 5 and 7 respectively). Therefore we fail to reject the null hypothesis of no remaining ARCH effects in the standardised residuals at any reasonable significance level. Therefore, the AR(9)-GARCH(1,1) model successfully captures the conditional variance dynamics of Google's daily returns as there is no evidence of leftover volatility clustering in the standardised residuals.

```{r q4b}
garch_summary <- capture.output(show(ar_garch_model))
start_idx <- which(garch_summary == "Weighted ARCH LM Tests")
ARCHLM_extract <- garch_summary[start_idx:(start_idx + 6)]
cat(ARCHLM_extract, sep = "\n")
```

### (c)

This "volatility" was greatest around early 2020, which aligns with the initial outbreak of the COVID-19 pandemic and associated global market turmoil. This lines up with the March 2020 equity crash, when investors reacted to lockdowns and uncertainty in earnings. 

```{r q4c}
vol_df <- tibble(date = data_est$date, sd = sigma(ar_garch_model))
ggplot(vol_df, aes(date, sd)) + geom_line() + labs(title="Conditional Volatility", y="sigma")

```

### (d)

The standardised residuals appear centered around zero with approximately unit variable, suggesting this model has correctly captured the conditional variance element of this process.

However, the residual distribution shows evidence of leptokurtosis, as indicated by the descriptive statistics: skewness of –0.294 and kurtosis of 7.26, which is more than double the Gaussian benchmark of 3.

Overall, this indicated that although volatility clustering has been filtered out, the residuals still exhibit non-normal behaviour in higher moments. 

Specifically, it has been unabel to deal with the heavy tails of this distribution, implying prediction intervals based on a normality assumption will understate the probability of extreme negative events (losses because of the slight negative skeweness of this distribution).



```{r q4d}
resid_std <- as.numeric(residuals(ar_garch_model, standardize = TRUE))

ggplot(tibble(resid_std), aes(resid_std)) +
  geom_histogram(aes(y = after_stat(density)), bins = 30, alpha = 0.6) +
  stat_function(fun = dnorm, args = list(mean = mean(resid_std), sd = sd(resid_std)), linewidth = 1) +labs(title="Distribution of AR(9)-GARCH(1-1) residuals")

tibble(
  Mean = mean(resid_std),
  SD = sd(resid_std),
  Skewness = mean((resid_std - mean(resid_std))^3) / sd(resid_std)^3,
  Kurtosis = mean((resid_std - mean(resid_std))^4) / sd(resid_std)^4
)
```


# Question 5

### (a)

```{r q5a}
fc <- forecast(ar9, h = 10)
fc_ar9 <- tibble(H = 1:10,
                 Forecast = as.numeric(fc$mean),
                 Lo95 = as.numeric(fc$lower[,"95%"]),
                 Hi95 = as.numeric(fc$upper[,"95%"]),
                 Actual = as.numeric(data_fcst$returns),
                 Error = Actual - Forecast)

print(fc_ar9)

```

### (b)

```{r q5b}
garch_fc <- ugarchforecast(ar_garch_model, n.ahead = 10)
fc_garch <- tibble(
  H = 1:10,
  Forecast = as.numeric(fitted(garch_fc)),
  SD_Forecast = as.numeric(sigma(garch_fc)),
  Lo95 = Forecast - 1.96 * SD_Forecast,
  Hi95 = Forecast + 1.96 * SD_Forecast,
  Actual = as.numeric(data_fcst$returns),
  Error = Actual - Forecast)
print(fc_garch)
```
### (c)

The AR(9)-GARCH(1,1) model is marginally preferred by virture of a lower RMSE.

```{r q5c}
rmse_ar9 <- sqrt(mean(fc_ar9$Error^2))
rmse_garch <- sqrt(mean(fc_garch$Error^2))
tibble(Model = c("AR(9)", "AR(9)-GARCH(1,1)"),
       RMSE = c(rmse_ar9, rmse_garch))
```
### (d)

* Both models produce similar point forecasts.

* However, their uncertainty assessments (as proxied by forecast intervals) differ. The AR model produces intervals which widen gradually with horizon due to accumulating error preidctions (based on the homoskedasticity assumption) whereas the AR-GARCH model's intervals are wider overall as a reaction to the elevated volatility observed in the final part of the sample. 


```{r q5d}
pdat <- bind_rows(
  tail(data_est,20) %>% transmute(date, y = returns, type = "Actual"),
  data_fcst %>% transmute(date, y = returns, type = "Actual"),
  fc_ar9 %>% mutate(date = data_fcst$date) %>% transmute(date, y = Forecast, type = "AR(9)"),
  fc_garch %>% mutate(date = data_fcst$date) %>% transmute(date, y = Forecast, type = "AR(9)-GARCH(1,1)")
)

ggplot()+
  geom_line(data = pdat, aes(date, y, color = type))+
  geom_ribbon(data = fc_ar9 %>% mutate(date = data_fcst$date), aes(date, ymin = Lo95, ymax = Hi95), alpha = .15, fill = "blue")+
  geom_ribbon(data = fc_garch %>% mutate(date = data_fcst$date), aes(date, ymin = Lo95, ymax = Hi95), alpha = .15, fill = "red")+
  labs(title = "Last 4 weeks (actual) + 10-step forecasts", x = "Date", y = "Returns")


```

### (e)

The boostrap prediction intervals are centred closer to the mean forecasts obtained in part (b), with median values closer to zero (consistent with the model's tiny conditional mean). 

However, the intervals themselves are narrower and slightly more stable across horizons. This is because this empirical approach to producing intervals produces is less dispersed than the theoretical normal assumption used in part (b).

Both approaches yield similar point forecasts, but the boostrap approach is more robust to the heavy-tailed/slight negative skewness of the residual distribution observed in Q4(d). This gives a more realistic measure of forecasts uncertainty under a violation of normality assumptions.

```{r q5e}
boot <- ugarchboot(ar_garch_model, method = "partial", n.ahead = 10, n.bootpred = 5000)

boot_pi <- tibble(
  H = 1:10,
  Median = apply(boot@forc@forecast$series, 2, median),
  Lo95 = apply(boot@forc@forecast$series, 2, quantile, probs = 0.025),
  Hi95 = apply(boot@forc@forecast$series, 2, quantile, probs = 0.975)
) %>% print()

```

### (f)

In question 4(d) the standardised residuals were found to be negatively skewed and leptokurtic. Therefore, we should expect the bootstrap prediction intervals to deviate from the symmetric Gaussian bands used in part (b).

The additional residual statistics defined below confirm this:

* The empirical lower 2.5% quantile (-2.13) is more extreme than the normal benchmark (-1.96), while the upper 97.5% quantile (1.85) is slightly lower than the normal benchmark (1.96). This indicated a heavier left tail and morem ass near zero consistent with negative skewness and high kurtosis.

* These explain the observed differences in interval shape and width. The normal intervals from part (b) are wide and symmetric (approximately 0.066 - 0.069) while the boostrap intervals are tighter (consistent 0.003 width). The narrower, stable boostrap intervals reflect the fact taht most resampled residuals fall close to zero thanks to the very large peaking of the empirical distribution (much greater than under normality).

* skewness = −0.29, kurtosis = 7.26, and the lower 2.5% quantile (q_emp_2.5) = −2.13 vs −1.96 under Normality (q_norm_2.5). These indicate a heavier left tail and more mass near zero, which explain why the bootstrap intervals are both tighter overall and slightly asymmetric on the lower side.


```{r q5f}

# Comparing prediction intervals
comp <- tibble(H = 1:10,
  Analytic_Width = fc_garch$Hi95 - fc_garch$Lo95,
  Bootstrap_Width = boot_pi$Hi95 - boot_pi$Lo95) %>% print()

# Additional statistics
e <- as.numeric(residuals(ar_garch_model, standardize = TRUE)); m <- mean(e); s <- sd(e)
skew <- mean((e - m)^3) / s^3
kurt <- mean((e - m)^4) / s^4
tails <- tibble(
  q_emp_2.5 = quantile(e, .025), q_emp_97.5 = quantile(e, .975),
  q_norm_2.5 = qnorm(.025),       q_norm_97.5 = qnorm(.975),
  Skewness = skew, Kurtosis = kurt) %>% print()


```
