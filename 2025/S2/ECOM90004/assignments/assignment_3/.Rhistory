library(tidyverse)
library(lubridate)
library(forecast)
library(urca)
library(rugarch)
theme_set(theme_minimal())
data <- read_csv("Google.csv")
glimpse(data)
data <- read_csv("Google.csv") %>%
select(date, price = Google) %>% # Google column to price
mutate(returns = diff(log(price)))
data <- read_csv("Google.csv") %>%
select(date = Date, price = Google) %>% # Google column to price
mutate(returns = diff(log(price)))
data <- read_csv("Google.csv") %>%
select(date = Date, price = Google) %>% # Google column name to price
mutate(price_log = log(price)) %>% # Creating log price series
mutate(price_dlog = price_log - lag(price_log))
data_raw <- read_csv("Google.csv") %>%
select(date = Date, price = Google) %>% # Google column name to price
mutate(price_log = log(price)) %>% # Creating log price series
mutate(price_dlog = price_log - lag(price_log)) %>%  # Creating dlog series
```
data_raw <- read_csv("Google.csv") %>%
select(date = Date, price = Google) %>% # Google column name to price
mutate(price_log = log(price)) %>% # Creating log price series
mutate(price_dlog = price_log - lag(price_log))  # Creating dlog series
install.packages("moments")
library(tidyverse)
library(lubridate)
library(forecast)
library(urca)
library(rugarch)
theme_set(theme_minimal())
# Arrange table by AICc descending and print
results <- results
pacf(data_est$returns)
library(tidyverse)
library(lubridate)
library(forecast)
library(urca)
library(rugarch)
theme_set(theme_minimal())
data <- read_csv("Google.csv", show_col_type = FALSE) %>%
select(date = Date, price = Google) %>%
mutate(price_log = log(price)) %>%
mutate(returns = price_log - lag(price_log)) %>%
na.omit()
data <- read_csv("Google.csv", show_col_type = FALSE) %>%
select(date = Date, price = Google) %>%
mutate(price_log = log(price)) %>%
mutate(returns = price_log - lag(price_log)) %>%
na.omit()
data_est <- data %>% slice(1:(n() - 10)) # Estimation sample
data_fcst <- data %>% slice((n() - 9):n()) # Testing sample
```
The analysis below indicates:
* The time series of daily google returns is a mean-reverting process around zero, but exhibits clear patterns of conditional volatility (non-constant variance).
* The histogram reinforces this, revealing a distribution which is more peaked and heavy-tailed than the fitted normal curve.
* Descriptive statistics reinforce these observations: a near-zero mean with high kurtosis and non-zero skewness indicates asymmetry and fat tails.
These findings point to non-constant/non-Gaussian volatilty in this process. This merits investigating using an AR-GARCH methodology to model it.
ggplot(data_est, aes(x = date, y = returns)) + geom_line() + labs(title = "Google daily returns",x = "Date", y = "Log Returns")
ggplot(data_est, aes(returns)) +
geom_histogram(aes(y = after_stat(density)), bins = 30, alpha = .6) +
stat_function(fun = dnorm, args = with(data_est, list(mean = mean(returns, na.rm=TRUE), sd = sd(returns, na.rm=TRUE))), linewidth = 1) +
labs(title = "Histogram of Daily Google Returns with Fitted Normal Distribution", x = "Returns", y = "Density")
stats <- data_est %>% summarise(
Mean = mean(returns, na.rm = TRUE),
SD = sd(returns, na.rm = TRUE),
Skewness = mean((returns - mean(returns, na.rm=TRUE))^3, na.rm=TRUE) / sd(returns, na.rm=TRUE)^3,
Kurtosis = mean((returns - mean(returns, na.rm=TRUE))^4, na.rm=TRUE) / sd(returns, na.rm=TRUE)^4
) %>%  print()
stats <- data_est %>% summarise(
Mean = mean(returns, na.rm = TRUE),
SD = sd(returns, na.rm = TRUE),
Skewness = mean((returns - mean(returns, na.rm=TRUE))^3, na.rm=TRUE) / sd(returns, na.rm=TRUE)^3,
Kurtosis = mean((returns - mean(returns, na.rm=TRUE))^4, na.rm=TRUE) / sd(returns, na.rm=TRUE)^4
) %>%  print()
data_est <- data_est %>%
mutate(weekday = wday(date, label = TRUE, abbr = TRUE))
model_dow <- lm(returns ~ weekday, data = data_est, contrasts = list(weekday = "contr.treatment"))
summary(model_dow)
pmax <- 15
AICc_vals <- LBp_vals <- numeric(pmax + 1)
for (p in 0:pmax) {
eq <- Arima(data_est$returns, order = c(p, 0, 0), include.mean = FALSE, method = "ML")
AICc_vals[p + 1] <- eq$aicc
LBp_vals[p + 1] <- Box.test(residuals(eq), lag = p + 4, type = "Ljung-Box", fitdf = p)$p.value
}
for (p in 0:pmax) {
eq <- Arima(data_est$returns, order = c(p, 0, 0), include.mean = FALSE, method = "ML")
AICc_vals[p + 1] <- eq$aicc
LBp_vals[p + 1] <- Box.test(residuals(eq), lag = p + 4, type = "Ljung-Box", fitdf = p)$p.value
}
best_p <- which.min(AICc_vals) - 1
cat("Selected AR order p =", best_p, "\nAICc =", round(min(AICc_vals), 3),
"\nLjung-Box p-value =", round(LBp_vals[best_p + 1], 4), "\n")
p <- 9
ar9 <- Arima(na.omit(data_est$returns), order = c(p,0,0), include.mean = FALSE)
autoplot(ar9) + ggtitle("AR(9) Roots")
ar_garch_model <- ugarchfit(
data = data_est$returns,
ugarchspec(
variance.model = list(garchOrder = c(1, 1)),
mean.model     = list(armaOrder = c(p, 0), include.mean = FALSE)))
ar_garch_model <- ugarchfit(
data = data_est$returns,
ugarchspec(
variance.model = list(garchOrder = c(1, 1)),
mean.model     = list(armaOrder = c(p, 0), include.mean = FALSE)))
coef_table <- round(coef(ar_garch_model), 4)
coef_table <- round(coef(ar_garch_model), 4)
coef_table
garch_summary <- capture.output(show(ar_garch_model))
start_idx <- which(garch_summary == "Weighted ARCH LM Tests")
ARCHLM_extract <- garch_summary[start_idx:(start_idx + 6)]
cat(ARCHLM_extract, sep = "\n")
resid_std <- as.numeric(residuals(ar_garch_model, standardize = TRUE))
vol_df <- tibble(date = data_est$date, sd = sigma(fit_garch))
ggplot(vol_df, aes(date, sd)) + geom_line() + labs(title="Conditional Volatility", y="sigma")
garch_summary <- capture.output(show(ar_garch_model))
start_idx <- which(garch_summary == "Weighted ARCH LM Tests")
ARCHLM_extract <- garch_summary[start_idx:(start_idx + 6)]
cat(ARCHLM_extract, sep = "\n")
vol_df <- tibble(date = data_est$date, sd = sigma(fit_garch))
vol_df <- tibble(date = data_est$date, sd = sigma(ar_garch_model))
ggplot(vol_df, aes(date, sd)) + geom_line() + labs(title="Conditional Volatility", y="sigma")
resid_std <- as.numeric(residuals(ar_garch_model, standardize = TRUE))
ggplot(tibble(resid_std), aes(resid_std)) +
geom_histogram(aes(y = after_stat(density)), bins = 30, alpha = 0.6) +
stat_function(fun = dnorm, args = list(mean = mean(resid_std), sd = sd(resid_std)), linewidth = 1) +labs(title="Distribution of AR(9)-GARCH(1-1) residuals")
tibble(
Mean = mean(resid_std),
SD = sd(resid_std),
Skewness = mean((resid_std - mean(resid_std))^3) / sd(resid_std)^3,
Kurtosis = mean((resid_std - mean(resid_std))^4) / sd(resid_std)^4
)
fc <- forecast(ar9, h = 10)
fc_ar9 <- tibble(H = 1:10,
Forecast = as.numeric(fc$mean),
Lo95 = as.numeric(fc$lower[,"95%"]),
Hi95 = as.numeric(fc$upper[,"95%"]),
Actual = as.numeric(data_fcst$returns),
Error = Actual - Forecast)
print(fc_ar9)
garch_fc <- ugarchforecast(ar_garch_model, n.ahead = 10)
fc_garch <- tibble(
H = 1:10,
Forecast = as.numeric(fitted(garch_fc)),
SD_Forecast = as.numeric(sigma(garch_fc)),
Lo95 = Forecast - 1.96 * SD_Forecast,
Hi95 = Forecast + 1.96 * SD_Forecast,
Actual = as.numeric(data_fcst$returns),
Error = Actual - Forecast)
garch_fc <- ugarchforecast(ar_garch_model, n.ahead = 10)
fc_garch <- tibble(
H = 1:10,
Forecast = as.numeric(fitted(garch_fc)),
SD_Forecast = as.numeric(sigma(garch_fc)),
Lo95 = Forecast - 1.96 * SD_Forecast,
Hi95 = Forecast + 1.96 * SD_Forecast,
Actual = as.numeric(data_fcst$returns),
Error = Actual - Forecast)
```{r q5b}
rmse_ar9 <- sqrt(mean(fc_ar9$Error^2))
rmse_garch <- sqrt(mean(fc_garch$Error^2))
tibble(Model = c("AR(9)", "AR(9)-GARCH(1,1)"),
RMSE = c(rmse_ar9, rmse_garch))
pdat <- bind_rows(
tail(data_est,20) %>% transmute(date, y = returns, type = "Actual"),
data_fcst %>% transmute(date, y = returns, type = "Actual"),
fc_ar9 %>% mutate(date = data_fcst$date) %>% transmute(date, y = Forecast, type = "AR(9)"),
fc_garch %>% mutate(date = data_fcst$date) %>% transmute(date, y = Forecast, type = "AR(9)-GARCH(1,1)")
)
ggplot()+
geom_line(data = pdat, aes(date, y, color = type))+
geom_ribbon(data = fc_ar9 %>% mutate(date = data_fcst$date), aes(date, ymin = Lo95, ymax = Hi95), alpha = .15, fill = "blue")+
geom_ribbon(data = fc_garch %>% mutate(date = data_fcst$date), aes(date, ymin = Lo95, ymax = Hi95), alpha = .15, fill = "red")+
labs(title = "Last 4 weeks (actual) + 10-step forecasts", x = "Date", y = "Returns")
boot <- ugarchboot(ar_garch_model, method = "partial", n.ahead = 10, n.bootpred = 5000)
boot_pi <- tibble(
H = 1:10,
Median = apply(boot@forc@forecast$series, 2, median),
Lo95 = apply(boot@forc@forecast$series, 2, quantile, probs = 0.025),
Hi95 = apply(boot@forc@forecast$series, 2, quantile, probs = 0.975)
) %>% print()
# (1) Compare analytic vs bootstrap intervals
anal_w <- fc_garch$Hi95 - fc_garch$Lo95
boot_w <- boot_pi$Hi95 - boot_pi$Lo95
comp <- tibble(
H = 1:10,
Analytic_Width = anal_w,
Bootstrap_Width = boot_w,
Width_Ratio = boot_w / anal_w,
Analytic_Asym = (fc_garch$Hi95 - fc_garch$Forecast) - (fc_garch$Forecast - fc_garch$Lo95),
Boot_Asym = (boot_pi$Hi95 - boot_pi$Median) - (boot_pi$Median - boot_pi$Lo95)
)
# (2) Residual normality diagnostics to explain differences (Q4d link)
e <- as.numeric(residuals(fit_garch, standardize = TRUE)); m <- mean(e); s <- sd(e)
# (2) Residual normality diagnostics to explain differences (Q4d link)
e <- as.numeric(residuals(ar_garch_model, standardize = TRUE)); m <- mean(e); s <- sd(e)
skew <- mean((e - m)^3) / s^3; kurt <- mean((e - m)^4) / s^4
JB <- length(e) * (skew^2/6 + (kurt - 3)^2/24); JB_p <- 1 - pchisq(JB, df = 2)
tails <- tibble(
q_emp_2.5 = quantile(e, .025), q_emp_97.5 = quantile(e, .975),
q_norm_2.5 = qnorm(.025),       q_norm_97.5 = qnorm(.975),
Skewness = skew, Kurtosis = kurt, JB = JB, JB_p = JB_p
) %>% print()
