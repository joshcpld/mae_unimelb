---
title: "Week 7 Tutorial"
author: 'Josh Copeland'
subtitle: 'ECON90033 - 2023 Semester 2 '
date: "Completed on 27 September 2023"


output: 
  html_document:
    theme: journal
---

```{r setup, include = FALSE}

library(tidyverse)
library(readxl)
library(lubridate)
library(forecast)
library(knitr)
library(lmtest)
library(FinTS)
library(rugarch)
library(xts)

```

# Exercise 1

## a) Draw a random sample of 200 from teh standard normal distribution using the rnorm() R function. Call this series nu.

```{r e1a, include = TRUE}

set.seed(654321)

nu <- ts(rnorm(200, mean = 0, sd = 1), start = 1)

plot.ts(nu, col = "blue")
abline(h=0)


```

## b) using nu, simulate an ACRH(1) error series

<br>

This task can be done in three steps:

* First, create the epa time series object for t = 1,...,200.
* Set the first epa value equal to zero.
* Finally, use a lop to calculate eps iteratively using your specified error equation.

After you've done this, plot the two series together.


```{r e1b, include = TRUE}

eps <- ts(start = 1, end = 200)

eps[1] <- 0

for (t in 2:200)
  #t is increased by 1 from 2 to 200, and in each step or iteration the expression in the {} brackets is evaluated.
{eps[t] = ts(nu[t] * sqrt(1 + 0.5*eps[t-1]^2))}


plot.ts(nu, col = "blue", ylab = "nu, eps", ylim = c(-6, 6), lty = 1)
abline(h=0)
lines(eps, col = "red", lty = 2)
legend("bottomright", bty = "n", legend = c("nu", "eps"), col = c("blue", "red"), lty = 1:2)

```


# c) Using eps, simualte the relevant AR(1) process. What do you observe?

Reviewing the chart below with the fomulas for the error and y in previous questions we can observe the following:

* Y is an AR(1)-ARCH(1) process: AR(1) conditional mean and and ARCH(1) error.
* this plot illustrates each unusually large shock in {v} is associated with a persistently large variation in the error and mean terms
* when this occurs, {y} tends to remain away from its unconditional mean: zero.

```{r e1c, include = TRUE}

y = ts(start = 1, end = 200)
y[1] = 0
for (t in 2:200)
{y[t] = 0.9*y[t-1] + eps[t]}

plot.ts(nu, col = "blue", ylab = "nu, eps", ylim = c(-14, 6), lty = 1)
abline(h=0)
lines(eps, col = "red", lty = 2)
lines(y, col = "darkgreen", lty = 3)
legend("bottomright", bty = "n", legend = c("nu", "eps", "y"),
col = c("blue", "red", "darkgreen"), lty = c(1,2,3))

```

# d) Suppose now that we don't know the data generation process for {y}. TDevelop its sample autocorrelation and partial autocorrelation functions. What do you observe?

<br>

* SACF is significant throughout all the lags and declines gradually/exponentially (at least up to the 6th lag)
* The first SPACF is significant, and apart from the rest none are statistically significant. Except for the 8th SPACF, which is probably just a statistical fluke (Type I error).

These patterns suggest that {y} can be modelled as an AR(1) process.

```{r e1d, include = TRUE}

ka = min(10, length(y)/5)
acf(y, lag.max = ka, plot = TRUE)
pacf(y, lag.max = ka, plot = TRUE)

```

# e) Fit an AR(1) model to {y} and check the correlograms of the residuals for lags 1,...,10.

<br>

The intercept is not significant, but the slope definitely is. Their correlograms do not have any pattern and only the coefficients at lag 7 appear significant.

<br>

Furthermore, the LB  test maintains the null hypothesis of no autocorrelation of orders 1,...,10.

<br>

Therefore, the residuals behave as a white noise, so we can tentatively accept the AR(1) model for the conditional mean.

```{r e1e, include = TRUE}

#library(forecast)
arma10 <- Arima(y, c(1,0,0))

summary(arma10)

#library(lmtest)

coeftest(arma10, df = arma10$nobs - length(arma10$coef))

k <- length(arma10$coef)
Box.test(arma10$residuals, type = "Ljung-Box", lag = 10, fitdf = k)

```


# f) Check the squared residuals for autocorrelation or orders 1,...,10. What can you say about the possibility of ARCH errors?

<br> 

We need to study the squared residuals to see whether the error term might be conditionally heteroskedastic. In particular, we need to assess of the conditional variance of the error term might be generated by an AR or ARMA process. IF yes, this should show up in the squarted residuals.

<br> 

In the charts below we can see there are several significant spikes on the correlograms, and the LB test rejects the null hypothesis of no autocorrelation of orders 1,...,10. So an ARCH or GARCH model seems to be warranted.

```{r e1f, include = TRUE}

acf(arma10$residuals^2, lag.max = 10, plot = TRUE)

pacf(arma10$residuals^2, lag.max = 10, plot = TRUE)

Box.test(arma10$residuals^2, type = "Ljung-Box", lag = 10, fitdf = k)

```

# g) Perform an ARCH LM test with lags 1, 5 and 10. What do these test suggest to you?

<br> 

We conduct an ARCH LM test below, where the p-values are sufficiently small to reject the null hypothesis of no ARCH effects of order 1, order 1-5 and order 1-10.

<br>

We test the LM test three times to see hwo robust it is to the lag length. This is very useful in practice! Would go much higher than this for lag lengths.

```{r e1g, include = TRUE}

#library(FinTS)

ArchTest(arma10$residuals, lags = 1)
ArchTest(arma10$residuals, lags = 5)
ArchTest(arma10$residuals, lags = 10)

```


# h) the ARCH LM tests suggest that an ARCH or GARCH model might be appropriate to model {y}. They do not provide real guidance though for model specification. Since in general it is a good idea to start with some relatively simple specification, estimate again an AR(1) model of {y} but assume this time the errors are generated by an ARCH(1) process.

<br>

For this course we use the rugarch package. Page 10 has detailed information about what all the different parameters means for all the steps of estimating a (G)ARCH model. There are two essential functions: ugarchspec() specifies the model; ugarchfit() estimates the specified model.

This printout is really long, so take it step by step. The following dotpoint indicate titles throughout the printout.

* Conditional variance dynamuics

  - This shows the ARFIMA (effectively ARIMA) where standard errors are calculated based on maximum likelihood/normal distribution and quasi maximum likelihood (QML) where errors are robust against normality. Qualitatively, there's no difference between these errors in this example.
  - This output shows the intercept is insignificant whereas the slop is significant - just like part (e). In the variance equation, both the intercept (omega) and the slops of the ARCH(1) term (alpha1) are significant.
  
* Information criteria

  - The actual values are irrelevant, and only informative relative to other valuesw computes for teh same dependent variable and from the same sample. Ignore for now.
  
* Weighted Ljung-box test on standardised residuals
  
  - This shows the results of weights Ljung-Box tests on the standardised residuals on the standardised squared residuals.
  - Each P-value is quite large, so the null hypotheses of no autocorrelation of i) order 1, ii) order 1-2, and (iii) orders 1-5 are maintened for the standardised residuals and their squared counterparts alike.
  - This means the AR(1)-ARCH(1) model is adequate.
  
* Weighted ARCH LM tests

  - This shows teh results of weighted LM tests for ARCH effects remaining in the standardised residuals.
  - They support the conclusion reached above: that the AR(1)-ARCH(1) model is sufficient. 
  - This is because each p-value is fairly large, so there are not ARCH effect left in the residuals.
  
<br>

We don't discuss the last through parts of the prinout, but its still important to understand their prupose and the conclusions they imply.

* Nyblom stability test
  - This test is for parameter stability (i.e. structural change in the data generating process)
  - R performed a joint test for all four parameters and four individual tests. If the observed statistic is larger than the critical value the null hypothesis (that the parameters are stable).
  - From this, we can see all parameters are stable.
  
* Sign bias test
  - Test for leverage effects: if negative/positive returns have different influence on future volatility.
  - All p=balue are above 0.099, so these tests provide further support to the AR(1)-ARCH(1) model.
  
* Adjusted PEarson goodness of fit test
  - These are chi-squared goodness of fit tests for the comparison of the empirical distribution of the standardized residuals and the chosen condition distribution of the error term, which in this case is normal.
  - Group is the number of equidistant class intervals. No matter whether we consider, 20, 30, 40 or even 50 class intervals, the null hypothesis of normal distribution is maintained.
  
  

```{r e1h, include = TRUE}

#library(rugarch)

spec1 = ugarchspec(mean.model = list(armaOrder = c(1,0), include.mean = TRUE),
                   variance.model = list(model = "sGARCH", garchOrder = c(1,0)),
                   distribution.model = "norm")

estimate1 = ugarchfit(spec = spec1, data = y, solver = "solnp")
estimate1

```

# i) Finallly, consider one more convenient feature of the rugard package. After having estimated a GARCH model, 12 inforamtive plots can be drawn out either all at once or one-by-one interactively.


```{r e1i, include = TRUE}

#to get only one of the 12 plots, replace "all" with the number of the required plot.

plot(estimate1, which = "all")



```


<br>
<br>
<br>
<br>

# Exercise 2

The objective of this exercise is to illustrate the application of GARCH modelling on some real data. Basically the same as Exercise 1 without all of th explanations.

## a) Read in the relevant data file

```{r e2a, include = TRUE}

getwd()

t8e2 <- read_excel("t8e2.xlsx")


```

## b) Prepare a time series plot of price


```{r e2b, include = TRUE}

first = as.Date("2000-01-04") - as.Date("2000-01-01") + 1
last = as.Date("2020-06-15") - as.Date("2020-01-01") + 1

df1 = data.frame(Date = seq(as.Date("2000-01-04"),
as.Date("2020-06-15"), by = 1))

df2 = t8e2 

df = left_join(df1, df2, by = "Date") 

time_series <- df %>%
  pull(Price)

PF <- ts(time_series, start = c(2000, first), end = c(2020, last), frequency = 365)

plot.ts(PF, main = "Closing price on B3", col = "red")

```


## c) Calculate and plot the daily log returns

<br> 

Returns seem to be centred around zero, but there are sequences of unsually large absolute values, inidicating volatility clustering.


```{r e2c, include = TRUE}



Price <- df2 %>%
  pull(Price)

R <- ts(log(Price / lag(Price)))

plot.ts(R, col = "blue")

```

## d) Develop the correlograms for the log returns. Consider lags 1-10.

All of the coefficient are fairly small in absolute value. However, both correlograms have significant spikes at 3 and 6.

However, there's no clear patter to them, therefore you might start with an ARIMA(0,0,0) model.

```{r e2d, include = TRUE}

R <- na.omit(R)

acf(R, lag.max = 10, plot = TRUE)
pacf(R, lag.max = 10, plot = TRUE)

```


## e) See whether the ARIMA(0,0,0) model fits best to the data.

This shows the ARIMA(0,0,3) with a non-zero mean produced the smallest AICc statistic. This statistic is smaller than ARIMA(0,0,0) but not by much.

<br>

Only one of these terms is significant, but it does belong to ma3, which supports the ARIMA(0,0,3) specification.

We also check if the residuals are serially uncorrelated. The p-value of the Box-Ljung test is large, meaning we can't even reject the null hypothesis at the 10 per cent level. Therefore, there is no autocorrelation of orders 1 to 10.

Therefore, we accept ARIMA(0,0,3) as the model for the mean of {R}.


```{r e2e, include = TRUE}

best.arima <- auto.arima(R, seasonal = FALSE, ic = "aicc",
approximation = FALSE, stepwise= FALSE, trace = TRUE)

summary(best.arima)
coeftest(best.arima, df = best.arima$nobs - length(best.arima$coef))

k = length(best.arima$coef)
Box.test(best.arima$residuals, type = "Ljung-Box", lag = 10, fitdf = k)

```


## f) to see whether an aRCH variance euqation is warranted by the data, perform the ARCH LM test for lags up to 5.

<br>

Below we run ARCH LM tests for lags 1 through find. All the p-values are very small, so each rejects the null hypothesis of no ARCH errors.

```{r e2f, include = TRUE}

#library(FinTS)

for (h in 1:5) {
print(ArchTest(best.arima$residuals, lags = h))
}

```

## g) To get some idea about hte variance equation, develop the correleograms for the squared residuals from the ARIMA(0,0,3) model. Consider lags 1-10.

<br>

Every coefficient is significant at the 5% klevel. In this case it is best to experiment with a parsimonious GARCH(1,1) variance equation.

```{r e2g, include = TRUE}

acf(best.arima$residuals^2, lag.max = 10, plot = TRUE)
pacf(best.arima$residuals^2, lag.max = 10, plot = TRUE)

```


## h) Fit an ARMA(0,3)-GARCH(1,1) model to {R}. Assume that error terms in the mean equation is normally distributions.

What does this long printout tell us:

* Regardless of if we use robust standard errors or not, mu and ma3 are significant for the mean equation. In the variance equation, omega, alpha1 and beta1 are all significant.

* The weighted LB tests maintain the null hypothesis of no autocorrelation of (i) order 1, (ii) orders 1-5 and (iii) orders 1-9, for the standardised residuals and standardised squared residuals - this supports the model specification.

* Additionally, the weighted ARCH LM tests also maintain the null hypothesis of no (Additional) ARCH effects.

<br>

However, the remaining results cast some doubt on the model:

* The joint Nyblom stability test rejects the null hypothesis that the parameter values are constant. This is also the coast the individual tests for omega and beta1, and at the 5% significant level on alpha1 too.

* the test the postiive sign bias rejects the null hypothesis that large and small positive shocks have the same impact on volatility, and the sign test for the joint (i.e. negative and positive) effect also rejects the null hypothesis that marge and small, in absolut value, shocks have the same impact on volatility. Therefore, this model does not account for the difference between large and small shocks.

* The adjusted chi-suqared goodness of fit tests reject the null, suggesting that the condition distribution of the error term is not normal, as assumed originally.


```{r e2h, include = TRUE}

#library(rugarch)

spec_v1 <- ugarchspec(mean.model = list(armaOrder = c(0,3), include.mean = TRUE),
                      variance.model = list(model = "sGARCH", garchOrder = c(1,1)),
                      distribution.model = "norm")

estimate_v1 = ugarchfit(spec = spec_v1, data = R)
print(estimate_v1)


```


## i) obtain two plots for further insights

* The first plot shows there's some discrepancy between the empirical distribution and the hypothesised standard normal distribution. But this discrepancy is not large at all - the histogram is pretty much symmetric.

* The second returns the news impact curve, which visualises the response of the variance to the negative and positive shock. However, this curve looks pretty much symmetricm providing no support to the sign bias test.

```{r e2i, include = TRUE}

plot(estimate_v1, which = 8)
plot(estimate_v1, which = 12)

```
