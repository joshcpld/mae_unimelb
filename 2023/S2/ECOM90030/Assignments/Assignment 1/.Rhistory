HPR_simple_per_cent <- (a1e1e_final / a1e1e_first - 1) * 100
HPR_log_per_cent <- (log(a1e1e_final) - log(a1e1e_first)) * 100
print(HPR_simple_per_cent)
print(HPR_log_per_cent)
```
# Exercise 2
```{r e2, echo=TRUE, out.width = '80%'}
ts_e2 <- function(x) {
ts(x, start = c(1946,12), end = c(1987,2), frequency = 12)
}
a1e2 <- read_excel("a1e2.xlsx")
```
## a) Plot the 2, 3, 4, 5, 6 and 9 months United States zero coupon yields on a single nicely customised (title, label, colour) time-series plot. Comment on the line chart.
_Figure 6_ below shows the monthly yield for zero coupon US bonds for maturities of 2 to 6 months, as well as 9 months from December 1946 to 1987 February. By looking at the chart, we can see that the returns of all the yields plotted moved very similarly regardless of their maturity. However, by referring to the legend we know that the lowest and highest maturity (2 and 9 month) bonds act as a lower and upper bounds for yields seen, as can be seen by the gap between the black and purple lines.
This series is likely non-stationary as its mean and variance change over time, and could likely also be defined as a random walk. However, more analysis is required to confirm these hypotheses. It also seems like there are significant amounts of autocovariance in these series.
```{r e2a, echo=TRUE, out.width = '80%'}
a1e2a <- a1e2 %>%
select(-dates) %>%
map(~ts_e2(.x))
ts.plot(a1e2a[["r2"]], main = "Figure 6: United States zero coupon yields", ylab = "Monthly yield (%)", col = 1)
lines(a1e2a[["r3"]], col = 2)
lines(a1e2a[["r4"]], col = 3)
lines(a1e2a[["r5"]], col = 4)
lines(a1e2a[["r6"]], col = 5)
lines(a1e2a[["r9"]], col = 6)
legend("topleft", legend = c("2 months","3 months","4 months","5 months","6 months","9 months"), col = 1:6, lty = 1)
```
## b) Compute the spreads on the 3-month, 5-month and 9-month zero coupon yields relative to the 2-month yield. Plot these spreads using a line chart and comment on their properties.
_Figure 7_ shows the spreads of 3, 5 and 9 months zero coupon bonds relative to the 2 month zero coupon bond between December 1948 to February 1987. The series graphed here also look to be nonstationary, even though the series all seem to oscillate around zero. While it would take further analysis to determine if the mean of each series or not, they all seem heteroskedastic. There does not seem to be a large degree of autocovariance in these series.
```{r e2b, echo=TRUE, out.width = '80%'}
a1e2b <- a1e2 %>%
select(r2, r3, r5, r9) %>%
mutate(r3_s = r3 - r2) %>%
mutate(r5_s = r5 - r2) %>%
mutate(r9_s = r9 - r2) %>%
select(r3_s, r5_s, r9_s) %>%
map(~ts_e2(.x))
ts.plot(a1e2b[["r3_s"]], main = "Figure 7: United States zero coupon yields spreads", ylab = "Spread relative to 2 months yield (%)", col = 1, ylim = c(-1, 3))
lines(a1e2b[["r5_s"]], col = 2)
lines(a1e2b[["r9_s"]], col = 5)
legend("topleft", legend = c("3 months","5 months","9 months"), col = 1:3, lty = 1)
```
## c Compare the graphs in parts (a) and (b) and briefly discuss the time series properties of yields and spreads.
<br>
Before comparing the graphs its helpful to summarise the key takeaways from the graphs seen in parts (a) and (b):
* _Figure 6_ seems to have a nonconstant mean and autocovariance and is heteroskedastic.
* _Figure 7_ oscillates around zero and may have a constant mean, does not seem to have a significant amount of autocovariance, but is heteroskedastic.
Therefore, it is clear from these points above that neither of these figures show weakly stationary series, because neither figure shows series that have a constant mean and variance. However, both are nonstationary for different reasons:
* _Figure 6_ is nonstationary because its mean clearly changes over time, and it also looks like variance is increasing over time (heteroskedastic).
* _Figure 7_ is nonstationary because it is heteroskedastic even though some of these series could potentially have a constant mean. Both a constant mean and variance is required for a series to be stationary.
# Exercise 3
```{r e3, echo=TRUE, out.width = '80%'}
ts_e3 <- function(x) {
ts(x, start = c(1990,4), end = c(2004,7), frequency = 12)
}
a1e3 <- read_excel("a1e3.xlsx")
```
## a) Plot the equity price and log returns of Microsoft and identify the large movements in its share value during the period of the dot-com crisis, which began on 10 March 2000 and led to very large falls in the equity value of Microsoft and technology stocks in general.
<br>
```{r e3a, echo=FALSE, out.width = '80%'}
a1e3a <- a1e3 %>%
select(Msoft) %>%
mutate(Msoft_r_log = (log(Msoft) - lag(log(Msoft))) * 100) %>%
map(~ts_e3(.x))
# ts.plot(a1e3a[["Msoft"]], main = "Figure 8: Microsoft equity price", ylab = "$", col = "blue")
knitr::include_graphics("a1e3a_i.png")
ts.plot(a1e3a[["Msoft_r_log"]],main = "Figure 9: Microsoft monthly log returns", ylab = "%", col = "blue")
```
## b) Estimate the CAPM model for Microsoft
```{r e3b, echo=TRUE,out.width = '80%'}
a1e3b <- a1e3 %>%
select(Msoft, SP500, Tbill) %>%
mutate(Msoft_r = Msoft / lag(Msoft) - 1) %>%
mutate(SP500_r = SP500 / lag(SP500) - 1) %>%
mutate(Tbill_r = Tbill / 12) %>%
mutate(Msoft_ER = Msoft_r - Tbill_r) %>%
mutate(SP500_ER = SP500_r - Tbill_r) %>%
select(Msoft_ER, SP500_ER) %>%
map(~ts_e3(.x))
a1e3b_model_CAPM <- lm(a1e3b[["Msoft_ER"]] ~ a1e3b[["SP500_ER"]])
summary(a1e3b_model_CAPM)
```
### i) Comment on the adjusted R2 statistic and on the F-test for the overall significance.
<br>
Referring to the model summary printout for the CAPM model estimated above:
* The R-squared value, the sample coefficient of determination, is about 33%. This suggest that in the sample we've used to estimate this linear regression model only accounts for around 33% of the variations of the excess returns on the Microsoft's stock.
* the p-value of the F-test of overall significance of this linear regression model is practically zero. Therefore, we can safely reject the null hypothesis atany reasonable significance level and conclude at any reasonable significant that the regression model is significant, the slope estimate is significantly different from zero and that the R-squared value is positively significant.
### ii) Interpret the point estimates of the α and β parameters.
<br>
Referring to the model summary printout for the CAPM model estimated above:
* The point estimstae of the α-risk of Microsoft is 0.016648. This means the monthly excess return on Microsoft stocks exceeds the excess return on the market portfolio by 0.016648.
* The point estimate of the β-risk of Microsoft is 1.494373, which means the Microsoft can be classified as an 'aggressive' stock. Practically, this means that Microsoft of more volatile than the market as a whole. According to this point estimate, its movements are on average approximately half-more (150%) than the movements in the whole market.
### iii) Plot the residuals and perform the Jarque-Bera test of normality. What do you conclude?
Referring to the output below from the Jarque-Bera test of normality below, the p-value is practically zero at 0.00000416. therefore we can safely reject the null hypothesis at any reasonable significance level and conclude that the stochastic error terms are normally distributed.
```{r e3biii, echo=TRUE,out.width = '80%'}
a1e3b_model_CAPM_residuals <- as_tibble(residuals(a1e3b_model_CAPM)) %>%
map(~ts_e3(.x))
plot.ts(a1e3b_model_CAPM_residuals[[1]], xlab = "Date", ylab = "Residual", main = "Residuals - Microsoft", col = "blue")
jarque.bera.test(a1e3b_model_CAPM_residuals[[1]])
```
## c) To capture the effects of the dot-com crisis construct 11 dummy variables for each month of the crisis beginning with March 2000 and ending in January 2001.6 For example, the first, the second and the last dummy variables are
```{r e3bc, echo=TRUE,out.width = '80%'}
a1e3c <- as_tibble(a1e3b) %>%
mutate(date = seq(as.Date("1990-04-01"), as.Date("2004-07-01"), by = "1 month")) %>%
mutate(I1 =)
```
### i)
### ii)
### iii)
### iv)
### v)
a1e3c <- as_tibble(a1e3b) %>%
mutate(date = seq(as.Date("1990-04-01"), as.Date("2004-07-01"), by = "1 month"))
View(a1e3c)
I1 = ts(ifelse(Date >= "2000-03-01" & Date < "2000-04-01" , 1, 0),
start = c(1990, 5), end = c(2004, 7), frequency = 12)
a1e3c <- as_tibble(a1e3b) %>%
mutate(date = seq(as.Date("1990-04-01"), as.Date("2004-07-01"), by = "1 month")) %>%
mutate(I1 = ifelse(date == "2000-03-01", 1, 0))
View(a1e3c)
a1e3c <- as_tibble(a1e3b) %>%
mutate(date = seq(as.Date("1990-04-01"), as.Date("2004-07-01"), by = "1 month")) %>%
select(date, Msoft_ER, SP500_ER) %>%
mutate(I1 = ifelse(date == "2000-03-01", 1, 0))
a1e3c <- as_tibble(a1e3b) %>%
mutate(date = seq(as.Date("1990-04-01"), as.Date("2004-07-01"), by = "1 month")) %>%
select(date, Msoft_ER, SP500_ER) %>%
mutate(I1 = ifelse(date == "2000-03-01", 1, 0)) %>%
mutate(I1 = ifelse(date == "2000-04-01", 1, 0)) %>%
mutate(I1 = ifelse(date == "2000-05-01", 1, 0)) %>%
mutate(I1 = ifelse(date == "2000-06-01", 1, 0)) %>%
mutate(I1 = ifelse(date == "2000-07-01", 1, 0)) %>%
mutate(I1 = ifelse(date == "2000-08-01", 1, 0)) %>%
mutate(I1 = ifelse(date == "2000-09-01", 1, 0)) %>%
mutate(I1 = ifelse(date == "2000-10-01", 1, 0)) %>%
mutate(I1 = ifelse(date == "2000-11-01", 1, 0)) %>%
mutate(I1 = ifelse(date == "2000-12-01", 1, 0)) %>%
mutate(I1 = ifelse(date == "2001-01-01", 1, 0))
a1e3c <- as_tibble(a1e3b) %>%
mutate(date = seq(as.Date("1990-04-01"), as.Date("2004-07-01"), by = "1 month")) %>%
select(date, Msoft_ER, SP500_ER) %>%
mutate(I1 = ifelse(date == "2000-03-01", 1, 0)) %>%
mutate(I1 = ifelse(date == "2000-04-01", 1, 0)) %>%
mutate(I1 = ifelse(date == "2000-05-01", 1, 0)) %>%
mutate(I1 = ifelse(date == "2000-06-01", 1, 0)) %>%
mutate(I1 = ifelse(date == "2000-07-01", 1, 0)) %>%
mutate(I1 = ifelse(date == "2000-08-01", 1, 0)) %>%
mutate(I1 = ifelse(date == "2000-09-01", 1, 0)) %>%
mutate(I1 = ifelse(date == "2000-10-01", 1, 0)) %>%
mutate(I1 = ifelse(date == "2000-11-01", 1, 0)) %>%
mutate(I1 = ifelse(date == "2000-12-01", 1, 0)) %>%
mutate(I1 = ifelse(date == "2001-01-01", 1, 0))
View(a1e3c)
a1e3c <- as_tibble(a1e3b) %>%
mutate(date = seq(as.Date("1990-04-01"), as.Date("2004-07-01"), by = "1 month")) %>%
select(date, Msoft_ER, SP500_ER) %>%
mutate(I1 = ifelse(date == "2000-03-01", 1, 0)) %>%
mutate(I2 = ifelse(date == "2000-04-01", 1, 0)) %>%
mutate(I3 = ifelse(date == "2000-05-01", 1, 0)) %>%
mutate(I4 = ifelse(date == "2000-06-01", 1, 0)) %>%
mutate(I5 = ifelse(date == "2000-07-01", 1, 0)) %>%
mutate(I6 = ifelse(date == "2000-08-01", 1, 0)) %>%
mutate(I7 = ifelse(date == "2000-09-01", 1, 0)) %>%
mutate(I8 = ifelse(date == "2000-10-01", 1, 0)) %>%
mutate(I9 = ifelse(date == "2000-11-01", 1, 0)) %>%
mutate(I10 = ifelse(date == "2000-12-01", 1, 0)) %>%
mutate(I11 = ifelse(date == "2001-01-01", 1, 0))
View(a1e3c)
a1e3b_model_CAPM <- lm(a1e3b[["Msoft_ER"]] ~ a1e3b[["SP500_ER"]], data = a1e3b)
summary(a1e3b_model_CAPM)
map(~ts_e3(.x)
a1e3c <- as_tibble(a1e3b) %>%
a1e3c <- as_tibble(a1e3b) %>%
mutate(date = seq(as.Date("1990-04-01"), as.Date("2004-07-01"), by = "1 month")) %>%
select(date, Msoft_ER, SP500_ER) %>%
mutate(I1 = ifelse(date == "2000-03-01", 1, 0)) %>%
mutate(I2 = ifelse(date == "2000-04-01", 1, 0)) %>%
mutate(I3 = ifelse(date == "2000-05-01", 1, 0)) %>%
mutate(I4 = ifelse(date == "2000-06-01", 1, 0)) %>%
mutate(I5 = ifelse(date == "2000-07-01", 1, 0)) %>%
mutate(I6 = ifelse(date == "2000-08-01", 1, 0)) %>%
mutate(I7 = ifelse(date == "2000-09-01", 1, 0)) %>%
mutate(I8 = ifelse(date == "2000-10-01", 1, 0)) %>%
mutate(I9 = ifelse(date == "2000-11-01", 1, 0)) %>%
mutate(I10 = ifelse(date == "2000-12-01", 1, 0)) %>%
mutate(I11 = ifelse(date == "2001-01-01", 1, 0)) %>%
map(~ts_e3(.x))
a1e3c <- as_tibble(a1e3b) %>%
mutate(date = seq(as.Date("1990-04-01"), as.Date("2004-07-01"), by = "1 month")) %>%
select(date, Msoft_ER, SP500_ER) %>%
mutate(I1 = ifelse(date == "2000-03-01", 1, 0)) %>%
mutate(I2 = ifelse(date == "2000-04-01", 1, 0)) %>%
mutate(I3 = ifelse(date == "2000-05-01", 1, 0)) %>%
mutate(I4 = ifelse(date == "2000-06-01", 1, 0)) %>%
mutate(I5 = ifelse(date == "2000-07-01", 1, 0)) %>%
mutate(I6 = ifelse(date == "2000-08-01", 1, 0)) %>%
mutate(I7 = ifelse(date == "2000-09-01", 1, 0)) %>%
mutate(I8 = ifelse(date == "2000-10-01", 1, 0)) %>%
mutate(I9 = ifelse(date == "2000-11-01", 1, 0)) %>%
mutate(I10 = ifelse(date == "2000-12-01", 1, 0)) %>%
mutate(I11 = ifelse(date == "2001-01-01", 1, 0)) %>%
select(-date) %>%
map(~ts_e3(.x))
a1e3c_model_CAPM_dummies <- lm(a1e3c[["Msoft_ER"]] ~ a1e3c[["SP_500"]] + sum(a1e3c[3:13]))
a1e3c_model_CAPM_dummies <- lm(a1e3c[["Msoft_ER"]] ~ a1e3c[["SP_500"]] + a1e3c[3:13])
a1e3c_model_CAPM_dummies <- lm(a1e3c[["Msoft_ER"]] ~ a1e3c[["SP_500"]] + a1e3c[[3:13]])
a1e3c <- as_tibble(a1e3b) %>%
mutate(date = seq(as.Date("1990-04-01"), as.Date("2004-07-01"), by = "1 month")) %>%
select(date, Msoft_ER, SP500_ER) %>%
mutate(I1 = ifelse(date == "2000-03-01", 1, 0)) %>%
mutate(I2 = ifelse(date == "2000-04-01", 1, 0)) %>%
mutate(I3 = ifelse(date == "2000-05-01", 1, 0)) %>%
mutate(I4 = ifelse(date == "2000-06-01", 1, 0)) %>%
mutate(I5 = ifelse(date == "2000-07-01", 1, 0)) %>%
mutate(I6 = ifelse(date == "2000-08-01", 1, 0)) %>%
mutate(I7 = ifelse(date == "2000-09-01", 1, 0)) %>%
mutate(I8 = ifelse(date == "2000-10-01", 1, 0)) %>%
mutate(I9 = ifelse(date == "2000-11-01", 1, 0)) %>%
mutate(I10 = ifelse(date == "2000-12-01", 1, 0)) %>%
mutate(I11 = ifelse(date == "2001-01-01", 1, 0)) %>%
select(-date) %>%
pmap(~ts_e3(.x))
a1e3c <- as_tibble(a1e3b) %>%
mutate(date = seq(as.Date("1990-04-01"), as.Date("2004-07-01"), by = "1 month")) %>%
select(date, Msoft_ER, SP500_ER) %>%
mutate(I1 = ifelse(date == "2000-03-01", 1, 0)) %>%
mutate(I2 = ifelse(date == "2000-04-01", 1, 0)) %>%
mutate(I3 = ifelse(date == "2000-05-01", 1, 0)) %>%
mutate(I4 = ifelse(date == "2000-06-01", 1, 0)) %>%
mutate(I5 = ifelse(date == "2000-07-01", 1, 0)) %>%
mutate(I6 = ifelse(date == "2000-08-01", 1, 0)) %>%
mutate(I7 = ifelse(date == "2000-09-01", 1, 0)) %>%
mutate(I8 = ifelse(date == "2000-10-01", 1, 0)) %>%
mutate(I9 = ifelse(date == "2000-11-01", 1, 0)) %>%
mutate(I10 = ifelse(date == "2000-12-01", 1, 0)) %>%
mutate(I11 = ifelse(date == "2001-01-01", 1, 0)) %>%
select(-date) %>%
map(~ts_e3(.x))
a1e3c_model_CAPM_dummies <- lm(Msoft_ER ~ SP500_ER + I1 + I2 + I3 + I4 + I5 + I6 + I7 + I8 + I9 + I10 + I11, data = a1e3c)
summary(a1e3c)
summary(a1e3c_model_CAPM_dummies)
a1e3c <- as_tibble(a1e3b) %>%
mutate(date = seq(as.Date("1990-04-01"), as.Date("2004-07-01"), by = "1 month")) %>%
select(date, Msoft_ER, SP500_ER) %>%
mutate(I1 = ifelse(date == "2000-03-01", 1, 0)) %>%
mutate(I2 = ifelse(date == "2000-04-01", 1, 0)) %>%
mutate(I3 = ifelse(date == "2000-05-01", 1, 0)) %>%
mutate(I4 = ifelse(date == "2000-06-01", 1, 0)) %>%
mutate(I5 = ifelse(date == "2000-07-01", 1, 0)) %>%
mutate(I6 = ifelse(date == "2000-08-01", 1, 0)) %>%
mutate(I7 = ifelse(date == "2000-09-01", 1, 0)) %>%
mutate(I8 = ifelse(date == "2000-10-01", 1, 0)) %>%
mutate(I9 = ifelse(date == "2000-11-01", 1, 0)) %>%
mutate(I10 = ifelse(date == "2000-12-01", 1, 0)) %>%
mutate(I11 = ifelse(date == "2001-01-01", 1, 0)) %>%
mutate(dummy_period = ifelse(date >= as.Date("2000-03-01") & date <= as.Date("2001-01-01"), 1 , 0)
a1e3c <- as_tibble(a1e3b) %>%
a1e3c <- as_tibble(a1e3b) %>%
mutate(date = seq(as.Date("1990-04-01"), as.Date("2004-07-01"), by = "1 month")) %>%
select(date, Msoft_ER, SP500_ER) %>%
mutate(I1 = ifelse(date == "2000-03-01", 1, 0)) %>%
mutate(I2 = ifelse(date == "2000-04-01", 1, 0)) %>%
mutate(I3 = ifelse(date == "2000-05-01", 1, 0)) %>%
mutate(I4 = ifelse(date == "2000-06-01", 1, 0)) %>%
mutate(I5 = ifelse(date == "2000-07-01", 1, 0)) %>%
mutate(I6 = ifelse(date == "2000-08-01", 1, 0)) %>%
mutate(I7 = ifelse(date == "2000-09-01", 1, 0)) %>%
mutate(I8 = ifelse(date == "2000-10-01", 1, 0)) %>%
mutate(I9 = ifelse(date == "2000-11-01", 1, 0)) %>%
mutate(I10 = ifelse(date == "2000-12-01", 1, 0)) %>%
mutate(I11 = ifelse(date == "2001-01-01", 1, 0)) %>%
mutate(dummy_period = ifelse(date >= as.Date("2000-03-01") & date <= as.Date("2001-01-01"), 1, 0))
View(a1e3c)
a1e3c <- as_tibble(a1e3b) %>%
mutate(date = seq(as.Date("1990-04-01"), as.Date("2004-07-01"), by = "1 month")) %>%
select(date, Msoft_ER, SP500_ER) %>%
mutate(I1 = ifelse(date == "2000-03-01", 1, 0)) %>%
mutate(I2 = ifelse(date == "2000-04-01", 1, 0)) %>%
mutate(I3 = ifelse(date == "2000-05-01", 1, 0)) %>%
mutate(I4 = ifelse(date == "2000-06-01", 1, 0)) %>%
mutate(I5 = ifelse(date == "2000-07-01", 1, 0)) %>%
mutate(I6 = ifelse(date == "2000-08-01", 1, 0)) %>%
mutate(I7 = ifelse(date == "2000-09-01", 1, 0)) %>%
mutate(I8 = ifelse(date == "2000-10-01", 1, 0)) %>%
mutate(I9 = ifelse(date == "2000-11-01", 1, 0)) %>%
mutate(I10 = ifelse(date == "2000-12-01", 1, 0)) %>%
mutate(I11 = ifelse(date == "2001-01-01", 1, 0)) %>%
mutate(dot_com_dummy = ifelse(date >= as.Date("2000-03-01") & date <= as.Date("2001-01-01"), 1, 0))
a1e3c <- as_tibble(a1e3b) %>%
mutate(date = seq(as.Date("1990-04-01"), as.Date("2004-07-01"), by = "1 month")) %>%
select(date, Msoft_ER, SP500_ER) %>%
mutate(I1 = ifelse(date == "2000-03-01", 1, 0)) %>%
mutate(I2 = ifelse(date == "2000-04-01", 1, 0)) %>%
mutate(I3 = ifelse(date == "2000-05-01", 1, 0)) %>%
mutate(I4 = ifelse(date == "2000-06-01", 1, 0)) %>%
mutate(I5 = ifelse(date == "2000-07-01", 1, 0)) %>%
mutate(I6 = ifelse(date == "2000-08-01", 1, 0)) %>%
mutate(I7 = ifelse(date == "2000-09-01", 1, 0)) %>%
mutate(I8 = ifelse(date == "2000-10-01", 1, 0)) %>%
mutate(I9 = ifelse(date == "2000-11-01", 1, 0)) %>%
mutate(I10 = ifelse(date == "2000-12-01", 1, 0)) %>%
mutate(I11 = ifelse(date == "2001-01-01", 1, 0)) %>%
mutate(dot_com_dummy = ifelse(date >= as.Date("2000-03-01") & date <= as.Date("2001-01-01"), 1, 0)) %>%
select(-date) %>%
map(~ts_e3(.x))
a1e3c_model_CAPM_dummies <- lm(Msoft_ER ~ SP500_ER + dot_com_dummy, data = a1e3c)
summary(a1e3c_model_CAPM_dummies)
a1e3c_model_CAPM_dummies <- lm(Msoft_ER ~ SP500_ER + I1 + I2 + I3 + I4 + I5 + I6 + I7 + I8 + I9 + I10 + I11, data = a1e3c)
summary(a1e3c_model_CAPM_dummies)
a1e3c_model_CAPM_dummies_residual <- as_tibble(residuals(a1e3c_model_CAPM_dummies)) %>%
map(~ts_e3(.x))
jarque.bera.test(a1e3c_model_CAPM_dummies_residual[[1]])
library(car)
linearHypothesis(model = a1e3c_model_CAPM_dummies, c("I1 = I2 = I3 = I4 = I5 = I6 = I7 = I8 = I9 = I10 = I11 = 0"))
linearHypothesis(model = a1e3c_model_CAPM_dummies, c("I1 + I2 + I3 + I4 + I5 + I6 + I7 + I8 + I9 + I10 + I11 = 0"))
library(tidyverse)
library(readxl)
library(forecast)
library(tsbox)
library(car)
options(scipen=999) #Get's rid of scientific notation
################################################################################
################################Exercise 1######################################
################################################################################
# A) calculate returns and excess returns for the CAPM model
e1 <- read_excel("C:/Users/joshc/Documents/2023S2/ECON90033/Tutorials/Week 3/t3e1.xlsx")
e1_analysis <- e1 %>%
select(Date, Exxon, SP500, Tbill) %>%
mutate(Exxon_r = Exxon / lag(Exxon) - 1)  %>%
mutate(SP500_r = SP500 / lag(SP500) - 1) %>%
mutate(Tbill_r = Tbill / 12) %>%
mutate(Exxon_ER = Exxon_r - Tbill_r) %>%
mutate(SP500_ER = SP500_r - Tbill_r)
Exxon_ER <- e1_analysis %>%
select(Exxon_ER) %>%
ts(start = c(1990, 4), end = c(2004, 7), frequency = 12)
SP500_ER <- e1_analysis %>%
select(SP500_ER) %>%
ts(start = c(1990, 4), end = c(2004, 7), frequency = 12)
ts.plot(SP500_ER)
#B) Estimate the CAPM simple linear regression for Exxon
CAPM_Exxon = lm(Exxon_ER ~ SP500_ER)
summary(CAPM_Exxon)
#What are the key points from this output?
#R^2 is 0.23, suggesting that this regression accounts for only 23% of the variations
#of the excess returns on the Exxon stocks
#There is A LOT  more detail, refer back here when doing assignment.
#-------------------------------------------------------------------------------
#B) Test whether the beta-risk if Exxon is significantly different from 1, implying
#That is does not track the market one-to-one
#You need to construct a one-sided F-test. To do this manually you need to construct
#a t-value and then compared this to teh t-critical value.
#However, there is an easy way to do it in R
library(car)
linearHypothesis(model = CAPM_Exxon, c("SP500_ER = 1"))
#What this test does it create a a general F-test that you can specify yourself
#In this instance we are generating an F-test where the null-hypothesis is that
#Beta_1 from our CAPM_Exxon model is equal to 1.
#In the output, the p-value is practically zero, therefore we can reject the null hypothesis
#And conclude that Exxon does not track the stock market one-to-one
#Re-do the previous exercise using log returns
e1_analysis <- e1_analysis %>%
mutate(Exxon_ER_log = (log(Exxon) - lag(log(Exxon))) - Tbill_r) %>%
mutate(SP500_ER_log = (log(SP500) - lag(log(SP500))) - Tbill_r)
Exxon_ER_log <- e1_analysis %>%
select(Exxon_ER_log) %>%
ts(start = c(1990, 4), end = c(2004, 7), frequency = 12)
SP500_ER_log <- e1_analysis %>%
select(SP500_ER_log) %>%
ts(start = c(1990, 4), end = c(2004, 7), frequency = 12)
CAPM_Exxon_log <- lm(Exxon_ER_log ~ SP500_ER_log)
summary(CAPM_Exxon_log)
#-------------------------------------------------------------------------------
#C)
#Consider the original CAPM and conduct residual analysis by:
#i) plotting the residuals
CAPM_Exxon_residuals <- ts(residuals(CAPM_Exxon),
start = c(1990, 4), end = c(2004, 7), frequency = 12)
plot.ts(CAPM_Exxon_residuals, xlab = "Date", ylab = "Residuals", main = "Residuals - Exxon", col = "violetred4") %>%
abline(h=0)
#The residuals fluctuate around zero and show no obvious pattern, except the variance seems large in second half of series
#ii) conduct the White test for heteroskedasticity
#Note: in this test H_0 = homoskedasticity & H_A = heteroskedasticity
library(lmtest)
bptest(CAPM_Exxon, ~ SP500_ER + I(SP500_ER^2))
#In this test the p-value is 0.5, therefore we cannot reject the null hypothesis.
#Therefore, this result does not support the observation made above, that variance appears larger in the second half
#iii) Perform the BG test for (H_0) no autocorrelation up to order 12 in the error term against
#(H_A) some 1-12 autocorrelation
bgtest(CAPM_Exxon, order = 12, type = "Chisq")
#Again, we cannot reject the null hypothesis. This implies the stochastic error terms might be serially uncorrelated
#iv) test for the normality of the error term. (H_0) normal distribution is tested against (H_A) non-normal distribution.
library(tseries)
jarque.bera.test(CAPM_Exxon_residuals)
#P-value is 0.073, therefore we maintain H_0 at the 5% level and stochastic error terms might be normally distributed
#v) LM test for ARCH error, (H_0) no first-order autocorrelation in the squared stochastic error terms is tested
#against (H_A) first-order autocorrelation in the squared stochastic error terms
library(FinTS)
ArchTest(CAPM_Exxon_residuals, lags = 1)
ArchTest(CAPM_Exxon_residuals, lags = 12)
#The p-value is about 0.70, too large again to reject H0 at the usual significance levels, so there is no evidence of ARCH errors.
#this is the same for ARCH errors 1-12.
#v) Now do the reset test for (H_0) correct functional form and (H-A) incorrect funtional form
resettest(CAPM_Exxon, power = 3, type = "fitted")
# The p-value is about 0.77, too large again to reject H0 at the usual significance levels, so the functional form of the model might adequate.
################################################################################
################################Exercise 2######################################
################################################################################
e2 <- read_excel("C:/Users/joshc/Documents/2023S2/ECON90033/Tutorials/Week 3/t3e2.xlsx")
e2_analysis <- e2 %>%
mutate(ENERGY_ER = ENERGY - RF) %>%
mutate(MKT_ER = MKT - RF)
ENERGY_ER <- e2_analysis %>%
select(ENERGY_ER) %>%
ts(start = c(1927, 1), end = c(2013, 12), frequency = 12)
MKT_ER <- e2_analysis %>%
select(MKT_ER) %>%
ts(start = c(1927, 1), end = c(2013, 12), frequency = 12)
SMB <- e2_analysis %>%
select(SMB) %>%
ts(start = c(1927, 1), end = c(2013, 12), frequency = 12)
HML <- e2_analysis %>%
select(HML) %>%
ts(start = c(1927, 1), end = c(2013, 12), frequency = 12)
FF3 <- lm(ENERGY_ER ~ MKT_ER + SMB + HML)
summary(FF3)
#------------------------------------------------------------------------------
#B)
#If Beta_2 and Beta_3 = 0 then FF3 model collapses to the basic. Test these restrictions
linearHypothesis(model = FF3, c("SMB = 0", "HML = 0"))
#We can reject the null hypothesis, and conclude this three-factor model better explains
#The movements than the basic CAP
#C) This three-factor model have similar sized Betas for B2 & B3 in absolute terms, but opposite signs.
#Test the restriction B2 + B3 = 0
linearHypothesis(model = FF3, c("SMB + HML = 0"))
#remember H_0: B2 + B3 = 0, H_a the inverse
#We cannot reject the null hypothesis, therefore we can replace the 3-factor model with a restrictire version.
#Estimate this new model
FF3_r <- lm(ENERGY_ER ~ MKT_ER + I(SMB - HML))
summary(FF3_r)
# Comparing both printouts, ther are practically the same in terms of goodness of fit and slope estimate. Therefore it is not binding.
################################################################################
################################Exercise 2######################################
################################################################################
e3 <- read_excel("C:/Users/joshc/Documents/2023S2/ECON90033/Tutorials/Week 3/t2e2.xlsx")
# A) Estimate the present value model
PRICE <- e3 %>%
select(PRICE) %>%
ts(start = c(1900, 1), end = c(2016, 9), frequency = 12)
DIVIDEND <- e3 %>%
select(DIVIDEND) %>%
ts(start = c(1900, 1), end = c(2016, 9), frequency = 12)
pvm <- lm(log(PzRICE) ~ log(DIVIDEND))
summary(pvm)
#B) Examine the properties of the estimated model by performing the following tests:
# i. Plot the ordinary least squares residuals and interpret their time series patterns.
pvm_residuals <- ts(residuals(pvm),
start = c(1900, 1), end = c(2016, 9), frequency = 12)
plot.ts(pvm_residuals,
xlab = "Date", ylab = "Residuals",
main = "Residuals - Present value model", col = "red")
abline(h = 0)
#There is clearly positive first order autocorrelation in the residuals
#Test for autocorrelation to make sure this is actually the case
# ii. Test for autocorrelation of orders 1 to 6.
bgtest(pvm, order = 6, type = "Chisq")
#P-value is basically zero, therefore we reject the null hypothesis. There is evidence of autocorrelation.
# iii. Test for second order ARCH errors.
ArchTest(pvm_residuals, lags = 2)
#Reject null hypothesis, there is evidence of first/second order ARCH errors
# iv. Test for normality of the residuals.
jarque.bera.test(pvm_residuals)
#P-value is too high to reject null, therefore the stochastic error terms might be normally distributed.
#C)
#In part a we estimated the PV model
#In tutorial 2, this specification is taken from the alleged lin r/ship b/wn the log of Price and dividends:
#   lnP_t = -lngamma + lnD
#Where gamma is the discount factor at time 1 and the slop parameter is beta = 1
#Test this restriction and test the result and retest if merited using a generalized F-test
linearHypothesis(model = FF3, c("SMB + HML = 0"))
#The p-value is zero, so we reject the null: the slope parameter is different from one.
#therefore stick to the unrestricted form.
linearHypothesis(model = FF3, c("SMB + HML = 0"))
linearHypothesis(model = a1e3c_model_CAPM_dummies, c("I1 = 0", "I2 = 0", "I3 = 0", "I4 = 0", "I5 = 0", "I6 = 0", "I7 = 0", "I8 = 0", "I9 = 0", "I10 = 0", "I11 = 0", ))
linearHypothesis(model = a1e3c_model_CAPM_dummies, c("I1 = 0", "I2 = 0", "I3 = 0", "I4 = 0", "I5 = 0", "I6 = 0", "I7 = 0", "I8 = 0", "I9 = 0", "I10 = 0", "I11 = 0"))
plot.ts(a1e3b_model_CAPM_residuals[[1]], xlab = "Date", ylab = "Residual", main = "Figure 10: CAPM model residuals - Microsoft", col = "blue")
plot.ts(a1e3c_model_CAPM_dummies_residual[[1]], xlab = "Date", ylab = "Residual", main = "Figure 11: CAPM model with Residuals with dummies - Microsoft", col = "blue")
abline(h=0)
---
title: "ECON90033 - 2023 Semester 2 - Assignment 1"
author: 'Tutorial class: Thursday 16:15'
subtitle: 'Josh Copeland (SID: 144772)'
date: "Generated on `r format(Sys.time(), '%B %d, %Y')`"
output:
html_document:
theme: journal
---
```{r packages, include=FALSE}
library(tidyverse)
library(knitr)
library(readxl)
library(janitor)
library(tseries)
library(car)
options(scipen=999) #Get's rid of scientific notation
```
# Exercise 1
```{r e1, echo=TRUE, out.width = '80%'}
ts_e1 <- function(x) {
ts(x, start = c(2018,7), end = c(2023,7), frequency = 12)
}
a1e1 <- read_excel("a1e1.xlsx")
